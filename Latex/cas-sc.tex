%% 
%% Copyright 2019-2024 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3c of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-sc document for 
%% double column output.

\documentclass[a4paper,fleqn]{cas-sc}
\usepackage{hyperref}
% If the frontmatter runs over more than one page
% use the longmktitle option.
%\documentclass[a4paper,fleqn,longmktitle]{cas-sc}

%\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[authoryear]{natbib}
\usepackage{csquotes}
\usepackage{algorithm2e}
\usepackage{placeins}
\usepackage{xcolor} 

%%%Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
%%%

% Uncomment and use as needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\usepackage{xcolor}   % put this in your preamble

% helper macro
\newcommand{\red}[1]{\textcolor{black}{#1}}

\usepackage[many]{tcolorbox}
\usepackage{xcolor}
\usepackage{varwidth}
\usepackage{environ}
\usepackage{xparse}
\usepackage{makecell}
\usepackage{tabularx}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{Cognitive Large Language Models}    
% Main title of the paper
\title [mode=title]{Improving Online Anti-Phishing Training Using Cognitive Large Language Models}

% Short author
\shortauthors{Malloy, Fang, Gonzalez}  

\author[1,2]{Tailia Malloy}[
            orcid=0000-0003-2512-984X
            ]
\ead{tailiamalloy@gmail.com}
\affiliation[1]{organization={Department of Social and Decision Sciences, Carnegie Mellon University},
            addressline={4815 Frew Street}, 
            city={Pittsburgh},
            postcode={15213}, 
            state={PA},
            country={USA}}
\affiliation[2]{organization={Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg},
            addressline={29, avenue J.F. Kennedy}, 
            country={Luxembourg }}
\author[3]{Fei Fang}[
            orcid=0000-0003-2256-8329
            ]
\ead{feifang@cmu.edu}
\affiliation[3]{organization={Department of Software and Societal Systems, Carnegie Mellon University},
            addressline={4665 5th Ave}, 
            city={Pittsburgh},
            postcode={15213}, 
            state={PA},
            country={USA}}
            
\author[1]{Cleotilde Gonzalez}[
            orcid=0000-0002-6244-2918
            ]
\ead{coty@cmu.edu}

% Here goes the abstract
\begin{abstract}
Training the public to recognize phishing emails is challenging because attackers increasingly use Large Language Models (LLMs) to craft more convincing scams. Although previous research suggests that fully LLM-generated phishing emails are easier for humans to identify, recent models like GPT-4 used in tandem with prompt engineering methods could generate phishing emails that are more difficult to detect. Various training techniques have been used to educate end users about the risks of phishing emails, but these methods may not address the challenges posed by LLM-generated phishing emails. To investigate these challenges, we conducted an experiment using multiple different methods that cyber attackers can use to develop phishing emails. We found that the most challenging emails to detect were those written by humans and then edited and stylized using code generated by an LLM. We also created an anti-phishing training platform to study a personalized learning approach using cognitive models. The basic idea, borrowed from tutoring systems, is to use a cognitive model based on Instance-Based Learning (IBL) Theory to trace the student's actions and predict each email's classification decision. The training protocol then selects educational examples that maximize the student's classification improvement. We also used the predictions of the IBL model to prompt the LLM to generate written feedback for the students. Our study shows that our training methods significantly improved the student's classification decisions. These results are highly relevant to researchers interested in the social impacts of LLM, their misuse, and mitigation strategies and to practitioners interested in novel anti-phishing training techniques.
\end{abstract}

\begin{keywords}
Online Training \sep 
Cybersecurity \sep
Large Language Models \sep 
Cognitive Models \sep 
\end{keywords}
\maketitle

\section{Introduction}
% What is the context to the problem, why is it important
Phishing emails are an example of \textit{social engineering} in which cyber attackers exploit the lack of knowledge of security principles by end users to perform specific tasks that achieve the attacker's goals \citep{salahdine2019social}. These goals are varied and include stealing confidential business or personal information, gaining privileged access to systems, directly taking money from banks, or installing malware on computers \citep{wang2020defining}. There are many subcategories of phishing emails, such as spear phishing that attempts to use victim's personal information to build trust; whaling that targets high-level executives for sophisticated campaigns; or Business Email Compromise that uses existing access to an email to launch a phishing campaign \citep{chiew2018survey}. Phishing and other forms of social engineering result in billions of dollars of lost revenue and theft worldwide each year \citep{kalla2023phishing}, and is associated with high costs of anti-phishing training to educate end users about best practices \citep{brunken2023properly}. One of the biggest challenges in cybersecurity is the human factor, the weakest link in security in general and social engineering in particular. The goal of anti-phishing training platforms is to ensure that students correctly identify phishing emails as dangerous and non-phishing or ham emails as benign \citep{singh2020makes}.

% What is the specific problem we are adressing 
The ability of adversaries to automate various tasks required in the process of launching phishing campaigns is a significant challenge to defending against these attacks. Phishing kits, for example, automatically create phishing emails, fake websites, and other resources \citep{milletary2005technical}. These kits can even be automatically deployed and managed through a business model called Malware-as-a-Service \citep{karapapas2020ransomware}, significantly facilitating cyber attacker tasks. Recent advances in phishing email design take advantage of modern machine learning techniques \citep{guembe2022emerging}, such as the automatic translation of phishing emails into a target language \citep{basu2019generating}, or the use of Large Language Models (LLMs) to produce novel phishing emails \citep{heiding2024devising}. Although the specific technologies used to generate and control phishing campaigns have evolved, social engineering techniques have remained mostly consistent, usually invoking a sense of urgency, making an offer and / or the use of a fake identity, among other methods \citep{stojnic2021phishing, singh2020makes}. More recently, cybercriminals have been using LLMs to design phishing emails, which pose a unique challenge to end users \citep{bethany2024large}, because this method can be quickly adapted and iterated to eventually fool the end user. This specific use of LLMs raises serious ethical concerns about the misuse of these models \citep{weidinger2021ethical}.  

% Previous related methods
One of the main defenses against cyber attacks from social engineering is the education of end users through online training \citep{aldawood2019reviewing}. Although LLMs represent a significant concern for cyberdefense, existing methods of online anti-phishing training typically do not include LLM generated emails and instead rely on the use of predefined data sets of phishing emails, either collected from online repositories of real phishing emails or designed by cybersecurity experts (for a review, see \citep{marshall2023exploring, nguyen2023learning, sarker2024multi}). Previous studies have assessed human ability to identify GPT-3-generated phishing emails \citep{sharma2023well}, and found that users were better able to categorize these LLM-generated emails compared to written emails from human experts. Additional research has tested the ability of GPT-4 to generate and detect phishing emails \citep{heiding2024devising, koide2024chatspamdetector}. In \citep{desolda2024apollo}, the authors applied LLM to detect phishing emails and provide natural language explanations as to why an email was phishing. %However, the emails used were not generated using LLM, and the students were not assessed on their ability to categorize these emails. 

Various methods have been developed to enhance training platforms, aiming to make education more generalizable and resilient to changes in the learning task. Cognitive models are particularly valuable for understanding student learning and decision-making processes. These models can inform educational platforms about the most effective strategies for training students. One notable application is the use of cognitive tutors in mathematics education, which help to improve the quality and effectiveness of online instruction \citep{koedinger2006cognitive, koedinger2007exploring, wan2023recommendation}. \red{More recent methods have used AI models with predictive models of student learning to improve the quality of AI use in educational contexts such as grading and feedback \citep{crompton2024affordances}. However, these approaches typically rely on relatively simple equations modeling broad learning trajectories \citep{benotti2017tool}. For a recent review of cognitive model applications in education see \citep{kirschner2024learning}.}

\red{These approaches differ from cognitive models that aim to replicate the underlying human cognitive mechanisms involved in learning. Related methods also leverage Reinforcement Learning (RL) models of human learning to support educational interventions \citep{fahad2023reinforcement}. One challenge with applying RL models in settings like ours is the high data requirements \citep{barto2021reinforcement}. This issue is often addressed by averaging across large datasets of human judgments and preferences, a technique also used to finetune LLMs \citep{christiano2017deep}. However, this averaging can reduce individualization and domain-specificity, especially for users with limited experience using LLMs \citep{ma2025should}. These limitations have raised concerns about the use of out-of-the box LLMs in domain specific education, such as chemical engineering \citep{tsai2023exploring}, medicine \citep{chan2025using}, and in higher education more broadly \citep{walczak2023challenges}.}

\red{While cognitive models have many benefits}, significant challenges persist when using cognitive models with complex stimuli, such as the natural language of phishing emails \citep{malloy2023generative}. Though recent methods have shown success in modeling human learning and decision making by integrating LLMs with cognitive models to represent complex stimuli \cite{malloy2025training}. Other related methods to improve online training have proposed the use of LLMs to provide natural language feedback \citep{kasneci2023chatgpt}, although these can have problems with lack of individualization \citep{eapen2023personalization}, which can make feedback difficult to learn \citep{nieminen2023feedback}. Although LLMs can produce written content that is judged as highly logical, plausible, understandable, and novel \citep{zhao2023more}, there are also concerns over so-called hallucinations during which LLMs produce plausible sounding but factually incorrect information \citep{huang2023survey}, this is a major concern for educational settings as giving students incorrect information would lead to poor educational outcomes. One potential solution to this issue is to chain the prompts between LLMs and users, by continually entering previous conversation messages into new LLM prompts \citep{wu2022ai}. Comparisons of content generated by an LLM alone and human + LLM working together have resulted in conflicting results of the efficacy of this approach in reducing hallucinations and producing more human-like content \citep{zhao2023more, mcdonald2024reducing}. 

\red{In summary, despite the growing body of research on anti-phishing training and the integration of AI techniques into cybersecurity education, some key gaps remain to be addressed. First, current online training platforms rarely incorporate LLM-generated phishing emails. As discussed above, most students do not assess the impact of LLM-generated emails on end-user learning or integrate them into a training system. Second, personalization is very important to improve phishing detection, and existing approaches often rely on static data sets and generalized feedback, which limits adaptability to evolving threats. Thus, there is a clear need for more dynamic, individualized, and cognitively-informed framework that integrates LLMs with cognitive models to enhance phishing detection training. This study aims to address these limitations by proposing a novel \textit{cognitive large language model} (CLLM) framework that takes advantage of both the generative power of LLMs and the personalization capabilities of cognitive models to improve the efficacy of anti-phishing training. }

The following section provides a detailed background on relevant research in on-line anti-phishing training, large language models, and cognitive models before proceeding into a description of our proposed framework. After this, two experiments are used to first compare the various methods for generating phishing emails using LLMs and secondly to evaluate different proposed methods of improving online anti-phishing training using a CLLM.  

\section{Background}
\subsection{Anti-Phishing Training}
Anti-phishing training is a proactive approach to address the concerns introduced by phishing emails, teaching students to identify these emails and monitoring how they react to those emails. Several factors have been identified as potential end-user behaviors that may be indicative of an increased susceptibility to phishing emails. Examples of these behaviors include an inability to identify unfamiliar risks \citep{downs2006decision}, a lack of knowledge of the URL structure and HTTPS icons \citep{downs2007behavioral,zhang2007phinding}, low attention and motivation \citep{kumaraguru2010teaching}, or high narcissism \citep{curtis2018phishing}. The ultimate goal of anti-phishing training is to allow a diverse and large population of end users to effectively identify and avoid being phished in their daily lives \citep{jampen2020don}, with many approaches being developed to improve educational outcomes between populations. 

In \citep{singh2019training}, the authors compared anti-phishing training in terms of false positives and false negatives between conditions with a 25\%, 50\%, and 75\% rate of phishing emails during training, suggesting that the optimal balance to improve both types of incorrect responses occurred using 50\% phishing emails. Several studies have compared the format of anti-phishing training examples, either using a platform that mimics real-world email clients or alternatives such as comic-based platforms \citep{sheng2007anti, kumaraguru2009school}, toolbars \citep{cranor2007phinding}, or games \citep{roepke2020pond}. Although these may be effective, research has suggested that the format is irrelevant to learning outcomes \citep{sheng2010falls}, with individual demographics being more relevant. 

A survey by Jampen et al. highlights several desired features of anti-phishing training, such as equitable training for a diverse population, as well as methods of improving existing methods by focusing on psychological human factors such as trust in authority and limitations in attention \citep{jampen2020don}. One useful target to enable training that can take these psychological factors into account is to personalize training, as students can feel more engaged and better served by customized materials \citep{schroeder2017advanced}. One method of personalization in training is through customized feedback, which takes into account specific learning progress or user personality traits, adjusting the feedback to compensate for their performance. Another commonly studied aspect of anti-phishing training is the use of warning systems \citep{egelman2008you} that can highlight when an email may be potentially dangerous, allowing a user to identify dangerous emails even when they are not identified as such \citep{egelman2008you}. This research has also been explored in the context of the location where anti-phishing warnings are added, placing warnings near potentially dangerous links \citep{petelka2019put}, or including a combination of audio and visual signals in warnings \citep{cooper2021heads}.

\red{
A recent model, Cyri, leverages a large language model (LLM) to provide natural language support for users in real-world scenarios, assisting them in evaluating incoming emails \citep{la2025cyri}. One strategy Cyri employs to enhance the quality of its support is analyzing individual emails and related documents to personalize its natural language communication. This is similar in motivation to our use of a cognitive model to predict individual behavior, however, Cyri does not rely on a history of user decisions to make predictions. Another key distinction lies in the application domain: Cyri operates in real-world, non-educational contexts where the true classification of an email is often unknown, whereas our approach is grounded in an educational setting where the correct category or outcome is already established.
}

\subsection{LLM Generated Phishing Emails} 
LLMs such as the Generative Pretrained Transformer 3 (GPT-3) \citep{brown2020language} have been evaluated in their ability to generate phishing emails compared to cybersecurity experts \citep{sharma2023well}. These results demonstrate lower performance in LLMs in the design of social engineering attacks compared to humans. However, these LLMs are constantly evolving, raising the question of the ability of newer models to design social engineering attacks \citep{langford2023phishing}. Although more advanced models may be able to produce more human-like text and have improved reasoning capabilities, they also have more advanced methods to prevent misuse, which may result in less convincing phishing emails \citep{roy2024chatbots}. These safety features are not perfect, and there are methods of rapid engineering to bypass these safety functions and produce results that would otherwise be prevented \citep{liu2023jailbreaking}. 

Regardless of the relative effectiveness of LLMs in generating phishing emails, these models demonstrate a significant concern for cybersecurity \citep{gupta2023chatgpt}. The simplicity of Generative AI tools makes them easy to apply to tasks such as writing phishing emails from scratch or stylizing existing phishing emails to look more convincing, potentially increasing their effectiveness \citep{sharma2023well}. Modern LLMs are even capable of producing code \citep{khan2022automatic}, such as Javascript, HTML, and CSS, \citep{lajko2022towards} that can create highly convincing emails that resemble real emails sent from many companies \citep{park2024ai}. This adds an additional layer to the potential misuse of LLMs in social engineering attacks, as writing code for realistic looking emails would normally take minutes or hours, and can be done in seconds with LLMs \citep{fakhoury2024llm}. %These two areas, writing original phishing emails and stylizing emails with HTML and CSS code, are the main focus of our experiment to investigate how users may be susceptible to social engineering attacks using emails designed by humans or LLMs. 

One method of reducing the potential harm of LLMs is to train end users to identify dangerous or misleading content generated by LLMs \citep{cao2023defending}. End-user training can be improved using their feedback to fine-tune existing LLMs to be more aligned with human uses \citep{bai2022training}. This can train models to avoid producing content that is designed to trick or scam users, such as phishing emails. An example of this is Reinforcement Learning with Human Feedback (RLHF) \citep{christiano2017deep}, which can be used to fine-tune LLMs \citep{ouyang2022training}. However, the effectiveness of this RLHF fine-tuning in preventing the generation of dangerous content is not perfect, and these safety measures can often be worked around with relatively simple prompt engineering \citep{chen2023unleashing}. This method of prompt engineering can require significant work from humans who are trying to avoid the safety features of LLM \citep{white2023prompt}. However, these time requirements can be avoided by training a separate model to learn how to force an LLM to produce a given type of content \citep{zou2023universal}. %In this work, we focus on using relatively simple prompt engineering method to approximate a cyberattacker that is attempting to learn to prompt an LLM to produce phishing emails without the use of these more advanced model training methods. 

\subsection{Cognitive Modeling}
\subsubsection{Instance-Based Learning Model}
The cognitive modeling method used in this work is inspired by Instance-Based Learning Theory (IBLT) \citep{gonzalez2003instance}. Cognitive models that rely on the mathematical mechanisms of IBLT are called Instance-Based Learning (IBL) models, and are used to predict human decisions using instances with a predefined set of relevant attributes determined based on the specifics of the task. \red{IBL models originate from the ACT-R cognitive architecture \citep{gonzalez2003instance, GONZALEZ2013262}. However, we do not employ the full ACT-R architecture, as its comprehensive set of cognitive processes is unnecessary for our purposes and can be cumbersome to integrate with an online training system. Instead, IBL models leverage the IBLT algorithm based on the Activation Equation, a key mechanism of declarative memory in ACT-R. This approach delivers comparable performance in modeling decisions from experience while remaining lightweight and practical for our application \citep{GONZALEZ2013262}. While ACT-R encompasses a wide range of cognitive processes—including motor control, attention, reasoning, and planning—IBL provides a streamlined alternative focused specifically on decision-making from experience. Furthermore, IBL models are implemented in Python \citep{MorrisonPyibl} making them significantly more accessible than the ACT-R framework, which is implemented in LISP.}

The applications of models based on IBLT are broad. Simple applications of IBL models include binary choice tasks \citep{gonzalez2011instance, lejarraga2012instance}, binary sequential decision making \citep{bugbee2022making}, and binary competitive games \citep{malloy2023learning}. But IBL models have also been applied to practical settings, including the prediction of human behavior in anti-phishing training tasks, although this has typically been done by assigning attributes to each email observed by students \citep{cranford2019modeling}. Other applications in the cybersecurity domain include cyberdefense \citep{cranford2020toward}, and cyber attack decision-making \citep{aggarwal2022designing}. 

Generally, IBL models work by creating a memory that is used to store previously experienced instances, each instance representing one time point of observing a state and making a decision. IBL models make selections of the actions available in decision-making tasks by assigning values to each option, with these values being determined by blending past experiences based on the similarity between the current instance and the instances held in memory. The following sections outline the mathematical foundation of IBL models, and give attention to the method of integrating these concepts into predicting student's categorization of emails in the anti-phishing training task.

\textbf{Activation}
An instance $i$ in memory $\mathcal{M}$, represents a choice option $k$ composed of features $j$ in the set of features $\mathcal{F}$ of environmental decision alternatives, and a utility value $u_i$. These options are observed in an order represented by the time step $t$ in which an instance occurred $\mathcal{T}(i)$. In calculating this activation, the similarity between the features of the observed choice option and the instances in memory is represented by summing over all attributes the value $S_{ij}$, which is the similarity of the attribute $j$ of instance $i$ to the current state. This gives the activation equation, derived from ACT-R \citep{anderson2014atomic}, as: 
 
\begin{equation}
A_i(t) = \ln \Bigg( \sum_{t' \in \mathcal{T}_i(t)} (t - t')^{-d}\Bigg) + \mu \sum_{j \in \mathcal{F}} \omega_j (S_{ij} - 1) + \sigma \xi
\label{eq:activation}
\end{equation}
The parameters that are set either by modelers or set to default values are the decay parameter $d$; the mismatch penalty $\mu$; the attribute weight of each $j$ feature $\omega_j$; and the noise parameter $\sigma$. The default values for these parameters are $(d,\mu,\omega_j,\sigma) = (0.5, 1, 1, 0.25)$. The value $\xi$ is drawn from a normal distribution $\mathcal{N}(-1,1)$ and multiplied by the noise parameter $\sigma$ to add random noise to the activation.    

\textbf{Probability of Retrieval}
The probability of retrieval represents the probability that a single instance in memory will be retrieved when estimating the value associated with an option. To calculate this probability of retrieval, IBL models apply a weighted soft-max function onto the memory instance activation values $A_i(t)$ giving the equation:
\begin{equation}
P_{i}(t) = \dfrac{\exp{A_i(t)/\tau}}{\sum_{i' \in \mathcal{M}_k}\exp{A_{i'}(t)/\tau}}
\label{eq:retrieval}
\end{equation}
The parameter that is either set by modelers or set to its default value is the temperature parameter $\tau$, which controls the uniformity of the probability distribution defined by this soft-max equation. The default value for this parameter is $\tau = \sigma \sqrt{2}$.


\textbf{Blended Value}
The blended value of an option $k$ is calculated at time step $t$ according to the utility values $u_i$ weighted by the probability of retrieval of that instance $P_i$ and summing over all instances in memory associated to that option $\mathcal{M}_k$ to give the equation:
\begin{equation}
V_k(t) = \sum_{i \in \mathcal{M}_k} P_i(t)u_i
\label{eq:blending}
\end{equation}

\textbf{Action Selection}
The method of selecting actions based on their blended value is typically to select the action that maximizes the blended value $a_t = \max_k T_k(t)$, though alternatives exist for producing probability distributions over actions such as the weighted soft-max function:

\begin{equation}
p(k_i|t) = \dfrac{\exp(V_{k_i}(t))^t}{\sum_{k_j \in K j \ne i} \exp(V_l(t)^t)}
\label{eq:action}
\end{equation}

The actions selected by IBL models can either be thought of as predicting the optimal action to select in a decision-making task, when the model is being used to simulate behavior, or they can be thought of as predicting what a specific human would do when presented with that choice, if the model is being used in model-tracing to predict their behavior. When predicting a specific human's behavior, the memory of IBL models is typically made up of the same instances that a human has experienced in the decision-making task. 

\subsubsection{Cognitive Modeling in Online Education}
Methods of applying cognitive models in online education and training seek to improve educational outcomes and better understand how students learn using these platforms. Applications of cognitive models have sought to understand how students can learn efficiently from visual information such as infographics \citep{damyanov2018role}, assess student engagement \citep{shukor2014predictive}, or assess cognitive load \citep{skulmowski2022understanding}. These cognitive models can be applied to the design of new and alternative online education and training methods using different presentation of information, higher engagement, and decreased cognitive load, in an effort to improve educational outcomes \citep{abuhassna2020development}. Due to the COVID-19 pandemic, many educational organizations were required to transition to online platforms \citep{aguilera2020college}, resulting in an increase in research of the quality difference between more traditional education and online education \citep{aguilera2021comparison}. Evidence from this line of research has suggested that the use of cognitive models can address some of the issues introduced by the transition from classroom to online learning \citep{wei2021assessment}.

One of the key motivations for the use of cognitive modeling in education is that learners benefit from different training methods and that students change the way they learn throughout training \citep{aleven2016instruction}. The constantly changing requirements of personalization in learning \citep{bernacki2021systematic} have been addressed through the use of \textit{cognitive tutors} designed using ACT-R models used to estimate the current performance ability and learning trajectory of an individual student, to design instruction in an individually personalized manner. This approach is referred to as \textit{knowledge tracing} \citep{koedinger2001cognitive} and has been applied to mathematics education \citep{ritter2007cognitive}, programming and geometry \citep{corbett2001cognitive}, and in real classrooms for high school and college algebra education \citep{ritter2007cognitive}. 

The method proposed in this work is an instance of knowledge tracing with cognitive tutors but is unique in two ways (1) through the use of email embeddings as the attributes of the IBL cognitive model performing the knowledge tracing and (2) through the addition of information from this IBL model into the prompting of an LLM for natural language feedback. A recent application of an IBL model in knowledge tracing has been demonstrated to be successful in the human-computer interaction setting, by performing model tracing on teammates to encourage better collaboration and adaptation to changing environments \citep{cranford2024personalized}. In this work, we apply the same approach onto the educational setting to perform knowledge tracing on students using IBL, and improve their educational outcomes. 

\subsubsection{LLMs in Cognitive Modeling}
There have been a variety of decision-making domains in which cognitive models have been integrated with generative models such as human transfer of learning \citep{malloy2023generative}, motor control \citep{taniguchi2022whole}, auditory learning \citep{beguvs2020generative}, multimodal learning \citep{ivanovic2018generative}, and most relevantly for this work, the detection of phishing emails \citep{xu2022modeling, malloy2024applying}. These results demonstrate that integration of cognitive models and generative models, such as LLMs, can be effective in predicting and understanding human behavior in contexts that alternative approaches could not, such as those with complex stimuli. Together, these applications demonstrate the usefulness of generative AI in cognitive modeling in general but leave open the question of the optimal method of integrating cognitive models with generative models for educational settings. 

The specific method for integrating cognitive models with generative models varies depending on the application. One category of integration is the use of LLMs as knowledge resources \citep{kirk2023exploiting} that can be queried for task-specific information by cognitive models. Another method is to use the representations formed by generative models as input to the state space in cognitive models, to allow the analysis of more complex visual, language, or auditory stimuli \citep{friston2020generative}. Other methods focus on training or fine-tuning LLMs themselves through the use of cognitive models \citep{wu2024cognitive}. This mirrors research in LLM applications that control fine-tuning through mechanisms inspired by cognitive architectures, such as Generative Agents \citep{park2023generative}. This approach controls fine-tuning by drawing inspiration from previous literature applying cognitive architectures on the design of autonomous simulated agents \citep{laird2001knows}. One of the main purposes of this work is to better understand the best methods to integrate generative models into cognitive models for the specific application of improving online education and training.

\red{In this work, we evaluate the usefulness in real-world settings of previous methods that leveraged LLM embeddings of phishing emails as attributes for an IBL model \citep{xu2022modeling, malloy2024applying}. This is done by using an IBL model with LLM formed representations of phishing emails to predict individual participant learning progress as the engage in an anti-phishing educational platform. Additionally, we extend this related work by strengthening the integration of IBL and LLM models into a Cognitive Large Language Model (CLLM) training framework. This is done by using the IBL model with LLM representations to make predictions of student learning progress in real time, and using these predictions as part of the prompting for an LLM model used to provide feedback. In this way, this work extends our understanding of the effectiveness of IBL models that use LLMs to predict student learning, and also evaluates the usefulness of these predictions in providing students natural language feedback. }

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{Figures/Fig1.pdf}
  \caption{Our proposed Cognitive Large Language Model (CLLM) anti-phishing training framework uses an integration of an IBL model for phishing classification and a LLM to produce a Cognitive Large Language Model.}
  \label{fig:Proposed}
\end{figure}

\section{Cognitive Large Language Model (CLLM) Training Framework}
The training framework aims to enhance online education by leveraging a cognitive model and knowledge tracing tailored to each student's unique learning progress. This approach identifies the most effective educational examples to present to students and provides personalized natural language feedback through the integration of LLMs. Overall, the goals are to 1) apply a cognitive model to the prediction of human student training progress and performance 2) determine the most effective method that cyber attackers may use to design phishing emails 3) compare the ability of students to identify LLM-written phishing emails based on their experience with phishing emails and AI chat bots, and 4) design and evaluate two methods for improving online education of phishing emails written by the most challenging application of LLMs by cyber attackers.

Figure \ref{fig:Proposed} shows a diagram of our proposed anti-phishing training framework. At the top of the figure, the Cognitive Large Language Model (CLLM) is formed by an integration of an IBL cognitive model and a GPT-4 LLM. This integration is formed by using LLM \textbf{email embeddings} as attributes of the IBL model, and with \textbf{feedback prompting} of the GPT-4 model with information on individual student performance and knowledge. Although CLLM predicts the progress of training and the current performance of an individual using knowledge tracking, it makes \textbf{behavior predictions} using each email from the database to find the single email that the student is more likely to categorize incorrectly. This is done by calculating the value of categorizing an email as phishing or ham and choosing the email with the highest value assigned to the incorrect category. This is the email that ultimately appears to the student, which is also sent to the CLLM to allow \textbf{contextual prompting}. The CLLM also receives the \textbf{behavior observed} from the student, which is used to update the IBL model knowledge tracing, and input into the GPT-4 model. This method of prompting is used to provide \textbf{dialog feedback} that is personalized, contextual, and takes into account their individual performance and learning progress. 

\subsection{IBL Email Representations}
Emails presented to students were generated entirely or partially by LLM, which means that relying on hand-crafted attributes would require expert hand-coding of these attributes. To avoid this issue, we instead use the embeddings formed by LLM when processing the email message, as detailed in the previous section. IBL models require the calculation of the similarity $S_{ij}$ of the $k$ attributes, which is calculated between the presented instance $i$ and the instance held in memory.  We use the cosine similarity $\text{CS}(x,x')$ of these embeddings as the similarity function of the IBL model. This gives the IBL similarity metric between two emails $i$ and $j$ as:

\begin{equation}
S_{ij} = \text{CS}(e(i), e(j)) = \dfrac{e(i)^Te(j)}{||e(i)||||e(j)||} 
\label{eq:representation}
\end{equation} 

where $e(\cdot)$ is the embedding generation function. \red{In the case of our IBL model that uses embeddings formed by accessing the OpenAI API `text-embedding-3-large' model, which form a vector of 3072 64-bit floating point numbers. These embeddings are pre-formed for all emails in our dataset and stored in a separate dataset that is accessed during the training experiment. As is done in previous methods \citep{xu2022modeling, malloy2022modeling}, these embeddings are used as the sole attribute for an IBL model that predicts student learning progress, and can give predictions of whether a student will correctly or incorrectly categorize any email in our dataset. Together this allows for the CLLM Teacher email selection algorithm to select from all available emails based on the method described in the next section.}

\subsection{LLM Feedback Prompting}
There are two different methods for prompting LLMs to provide feedback to students. The LLM-feedback prompt uses only information from the emails shown to students and their behavior, while the CLLM-feedback prompt incorporates information from the IBL model. Figure \ref{fig:Prompting} shows the two different feedback methods to prompt the LLM to provide feedback to the student. \red{This design allows us to test whether IBL model predictions of student learning progress are useful for the prompting of a separate LLM that provides feedback to participants.}

\subsection{CLLM Teacher Email Selection Algorithm}
The selection of emails using the CLLM is done to maximize the difficulty of training examples and has been shown through simulation to improve training outcomes \citep{malloy2024leveraging}. Since half of the emails shown to students are phishing and the other half are spam, the algorithm first determines which email will be shown on the current trial and limits the difficulty maximization to only those emails. Additionally, to prevent emails from being reused multiple times, previously used emails are not included in the maximization. Then, for each email remaining, the CLLM knowledge trace is used to estimate the student's current value of categorizing an email as being ham or phishing. This allows the model to calculate the value for each email of an incorrect categorization, using the phishing categorization for ham emails and the ham categorization for phishing emails. This incorrect categorization value is assigned to each email and then the maximum of this value is chosen for all emails under consideration. This gives the next email to show as $e_i$ from the set of all possible emails $E$ as:

\begin{equation}
e_i = \max_{j \in E} \{V_{k_{e_j}}(t) | k_c \neq i_c \} 
\label{eq:similarity}
\end{equation} 
where $V_{k_{e_j}}(t)$ is the blended value of the instance of categorizing the email $e_j \in E$, and $ k_c \neq i_c$ represents the stipulation that the categorization is incorrect. This method of maximizing the probability of an incorrect response is motivated by the idea of keeping educational examples varied and avoiding the repetition of skills that are already mastered by students. 


\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig2.pdf}
  \caption{Left: Base feedback prompting from the LLM that does not include information from the IBL cognitive model, this base feedback is also used as the beginning of the prompt in the CLLM-feedback conditions. Right: Additional information from the IBL cognitive model that is included in the CLLM-feedback conditions. Orange strings of text signify variables that were altered for each new feedback instance based on the context of the current participant's learning progress.}
  \label{fig:Prompting}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/Fig3.pdf}
  \caption{Examples of feedback presented to real human students from the three different types of feedback.}
  \label{fig:Feedback}
\end{figure}

Figure \ref{fig:Feedback} shows three examples of feedback that students can see, as well as questions that they can ask to the chat bot teacher and the teacher's replies. By comparing the responses of the CLLM-feedback methods and the GPT-4 feedback method, we can see an example of the difference that adding IBL information to the prompts has. In the GPT-4 feedback example, the student is concerned that the website address appears illegitimate and could be a sign of a phishing attempt. Most likely, this is due to the long string of random numbers and letters at the end of the URL. However, the GPT-4 feedback chatbot focuses on the lack of HTTPS in the linked URL. Meanwhile, when a student asks the CLLM-feedback chatbot a question about the legitimacy of the sender of the email, the model correctly clarifies that the email sender can be legitimate while other features of the email signify that it is dangerous. This is due to the method of prompting the GPT-4 model with information on the specific features that individual students are more or less likely to identify as signaling that an email is dangerous. 

\section{Experimental Hypotheses and Methods}
The design of this experiment is structured to perform an initial study (Experiment 1) of the difficulty of categorizing different types of phishing and ham emails, and a follow-up study (Experiment 2) testing methods of improving training outcomes in anti-phishing training. In addition, an ablation experiment is run to compare the importance of connecting the IBL and GPT-4 models in our CLLM framework. The structure of this two-experiment process allows for an initial determination of what the relative difficulty of email authors and styles is and their interaction. From this analysis the most difficult condition or conditions were to be used in a follow-up second experiment testing two different methods for improving anti-phishing education. All experiments utilized the same online training platform shown in Figure \ref{fig:interface}. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig5.pdf}
  \caption{Screenshot example from the educational interface.}
  \label{fig:interface}
\end{figure}

%2) Experimental Design    
\subsection{Experimental Design}
\red{In each condition of the experiment, participants used their personal computer to navigate to our experimental website through a link on the Amazon Mechanical Turk website. The first page presented participants with information on the study and the consent form for participation. After providing consent, participants read instructions on the anti-phishing task and information related to basic features of phishing emails. Following this, a questionnaire was filled out on the participant's experience with AI chat bots and phishing attempts.}

\red{The first trials of the main experiment proceeded this and the phishing email, feedback area, and response area were shown to the participant. In all trials of the experiment, participants controlled when to move to the next stimuli and when to categorize their current stimuli. Participants completed ten trials without any feedback on the correctness of their responses, then 40 trials with feedback, followed by a final ten trials without feedback. After the main experiment, participants performed a post-experiment questionnaire on their perception of the content they observed as AI written. Finally, participants were informed of the bonus payment they would receive and left the experimental website.}

\subsubsection{Experiment 1}
The first independent variable of experiment 1 (the top axis of the table on the left in Figure \ref{fig:Conditions}) is the style of the email shown to students. Emails can be plaintext (plaintext) or styled using HTML and CSS code generated by GPT-4 (LLM). The Email Style measure can be either plaintext-styled or LLM-styled. 

The second independent variable of experiment 1 (the left axis on the left table in Figure \ref{fig:Conditions}) is the author of the email. Emails can be authored by human cybersecurity experts (Human) or generated from scratch by GPT-4 (LLM). The email author measure can be either human-written or LLM-written.

\subsubsection{Experiment 2}
The first independent variable of experiment 2 (the top axis of the middle table in Figure \ref{fig:Conditions}) is the method to provide feedback to students after their categorization decision. The feedback method can be either in the form of points (-1 or 1) or written by the CLLM model. The feedback method can be point feedback or CLLM-feedback. 

The second independent variable of experiment 2 (the left axis on the middle table in Figure \ref{fig:Conditions}) is the method of selecting emails to show to students. Email selection can be selected by the CLLM model or randomly selected. The email selection method can be either random or CLLM.

\subsubsection{Ablation Experiment}
In the ablation experiment, we compare our CLLM model that uses a connection between the IBL cognitive model and the GPT-4 LLM against the same training paradigm that does not have a connection between the IBL and GPT-4 models. This results in a condition that used IBL with email attributes instead of email embeddings to select emails, and an LLM to provide feedback without any information from the IBL model. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.95\linewidth]{Figures/Fig4.pdf}
  \caption{Representation of the differences between experimentation conditions. Lined background conditions were not run. The gray shaded conditions in experiment 1 and experiment 2 are the same set of participant data. Additionally, the black shaded conditions in experiment 2 and the ablation experiment are the same. }
  \label{fig:Conditions}
\end{figure}

\subsection{Hypotheses}
The first two hypotheses in our experiments preregistration are regarding the relative difficulty in identifying emails that are generated by humans or GPT, and emails that are styled by humans or GPT.\\
\textbf{Hypothesis 1:} The post-training accuracy and training improvement will be lowest in LLM-written LLM-styled emails, followed by human-written LLM-styled emails, then LLM-written plaintext-styled and finally highest in human-written plaintext-styled emails. \\
\textbf{Hypothesis 2:} Participants with more experience with phishing emails and AI chatbots will have higher categorization accuracy compared to students with less experience with phishing emails and AI chatbots. 

The final two hypotheses in our experiments preregistration are regarding the relative benefit of using the CLLM model to select emails for educational examples and provide natural language feedback to the students. \\
\textbf{Hypothesis 3:} The post-training accuracy and training improvement will \red{be the} highest in the CLLM-selected emails and the CLLM-feedback, then the randomly selected emails and the CLLM-feedback, then the CLLM-selected emails and the feedback of points, and lowest in the condition with the feedback of points and random emails.\\
\textbf{Hypothesis 4:} We expect that there will be a significant difference in post-training performance and biases when altering the email selection and feedback method. 

%3) Participant population description  
\subsection{Participants}
We recruited 417 students online through Amazon Mechanical Turk (AMT). We advertised Human Intelligence Tasks (HITs) for between 50-60 students for each condition. \red{Participants were required to reside in the United States of America, and be fluent in English as all the materials were provided in English. The selected participants had completed more than 100 HITs with at least a 95\% success rate. Participants were required to be over the age of 18, the mean age was 40.66$\pm$10.88 years. There were 270 Males, 144 Females, 1 Non-Binary Person, and 2 people who declined to answer.} After removing students who did not complete all the trials and those whose performance was less than two standard deviations below the mean of categorization accuracy. we were left with 382 students. The breakdown of students by experiment condition was LLM-written plaintext styled: 55; LLM-written LLM-styled: 54; human-written plaintext-styled: 52; human-written LLM-styled: 54; point feedback CLLM-selection: 42; CLLM-feedback random-selection: 44; CLLM-feedback CLLM-selection : 43; Human Attributes LLM Prompting: 38.

%4) Procedure 
\subsection{Procedure}
In all conditions of the experiments, the students began by confirming their consent to collect their \red{classifications of emails, and in the case of the conditions that allowed communication with an AI chatbot, the collection of these communications. Participants were also told in the case of AI chatbot conditions that their messages would be sent to an external website and could potentially be stored. In addition, the participants received} a description of the experiment being carried out and IRB approval. \red{As a part of data privacy assurance, participants were instructed not to message the AI chatbot with anything other than questions regarding phishing emails. Additionally, messages were first filtered for relevance by prompting GPT-4o to categorize them as relevant to anti-phishing education, and irrelevant messages were deleted. More information on the prompting for GPT-4o for this task is included in later sections.}

Participants then read instructions on the anti-phishing training task they were to perform and the payment they would receive. Before proceeding to the main experiment, students completed a 6 question quiz designed to assess their individual experience with phishing emails. During the main experiment, students were first presented with 10 randomly selected emails (5 ham and 5 phishing, randomly) without feedback on their accuracy to determine their baseline performance on the task. After this pre-training period, the participant conducted the main training trials of the experiment, consisting of 40 trials (20 ham and 20 phishing, randomly). Following the training period, there was a post-training period again without feedback consisting of 10 randomly selected emails (5 ham and 5 phishing, randomly), to compare the performance before and after the training period. 

An example of the training interface is shown in Figure \ref{fig:interface}, with more details of this interface below. After this post-training period, a post-experiment survey was administered that asked students questions about their perception of the emails they observed that were generated by an AI chatbot. This concluded the main experiment, after which the students were presented with a code they could use to signal that they had completed the task on the MTurk website and received payment. An example of the interface of the Phishing Classification Task is shown in Figure \ref{fig:interface}.

Experiment 1 varied the author of emails shown to students, either humans or LLM, as well as the style of the emails, either plaintext or HTML/CSS code generated by LLM. This comparison allowed us to determine which combination of human and GPT contributions to creating phishing emails was the most challenging to learn from human students. In the results section, we present a statistical analysis that demonstrates that the human-authored and LLM-styled emails were the most challenging for the students, meaning that condition 4 served as the basis for all conditions tested in experiment 2.

The structure of the experiments was established to select the most challenging condition in experiment 1, and test various methods of improving learning outcomes in that condition in experiment 2. These different methods for improving participant learning outcomes make up the different conditions for experiment 2, which varied the method of selecting emails to show to students, either random or with an IBL model, as well as the feedback presented, either 1 or -1 points, or a conversation with a CLLM chatbot. 

Both experiment protocols and pre-registrations, as well as all code for running the experiment website, data from students, statistical analysis, and graph generation code are available on OSF\footnote{\url{https://osf.io/wbg3r/}} and GitHub \footnote{\url{https://github.com/TailiaReganMalloy/foundation-phishing}}.  In addition to these seven conditions, we also conducted an additional ablation experiment that used feedback from the LLM model alone, without any added information from the IBL cognitive model. This is treated as an ablation condition to test the impact of integrating the IBL and LLMs as opposed to using only an LLM in the task.

%5) Dependent measures.
\subsection{Dependent Measures}
\textbf{Training Improvement}: is measured by the difference between the pre-training and post-training categorization accuracy measures. Since there are 10 pre-training and 10 post-training trials, the value of the training improvement ranges from -100 to 100 percentage points, in steps of 10. \\
\textbf{Training Speed}: is measured by the number of training trials that students take to achieve 80\% performance in a rolling window of 10 trials. This measure is calculated in this way to estimate the length of time it takes to achieve good performance during training, and defined in a way that makes a higher value better, for easier visual comparison across measures. \\
\textbf{AI Generation Perception}: is measured by the percentage of emails students report to be AI written in the post-experiment questionnaire (included in the Appendix). After showing the final email to the students, the students complete a response to 4 questions about their perception of the emails they saw as AI written. The percentage of emails they report to be AI written serves as a measure of their perception of observed emails as AI generated. These questions are included in the supplementary material. \\
\textbf{Phishing Knowledge}: is measured by the percentage of phishing quiz questions (included in the Appendix) students get correct in the pre-experiment quiz. Before the first email is shown to students, they completed a quiz on 6 questions detailing the attributes of phishing emails. The number of correct quiz responses serves as a measure of the baseline knowledge of the participant about phishing. These quiz questions are included in the supplementary materials. 

\section{Results}
\subsection{Experiment 1}
\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig6.pdf}
  \caption{Training improvement and training speed measures for each condition in experiment 1. For both measures, higher represents better training outcomes for students. Error bars represent standard deviation. * represents p<0.05.}
  \label{fig:trainingExp1}
\end{figure}
\subsubsection{Training Improvement.}
The left side of Figure \ref{fig:trainingExp1} compares the average improvement in participant training between each of the four conditions of experiment 1. From this we can see that the human-written LLM-styled condition had the lowest average training improvement. Both the human-written LLM Styled and LLM-written plaintext-styled conditions had significantly lower training improvement compared to the condition with the largest training improvement, the LLM-written LLM-styled condition. We perform the following statistical analysis to confirm the difficulty of the human-written LLM-styled condition in terms of training improvement. 

A two-way ANOVA was performed to compare the effect of the author and the style on the improvement of the training, which revealed that there was a statistically significant difference in the improvement of the training between the author (F (1,212) = 8.566, p = 0.004, $\eta^2$ = 0.039), no difference between style (F (1,212) = 0.092, p = 0.762, $\eta^2$ = 0) and an interaction effect (F (1,212) = 9.640, p = 0.002, $\eta^2$ = 0.043). Tukey's HSD Test for multiple comparisons found that the mean value of training improvement was significantly lower in the human-written GPT-4 styled condition compared to the written GPT-4 styled condition (p=0.022, diff=-10 se=3.462, T=-2.888, hedges=-0.579) and was significantly lower in the human-written GPT-4 styled condition compared to the GPT-4 written GPT-4 styled condition (p = 0.022, diff=9.098 se=3.446, T=2.640, hedges=0.491). 

\red{From these results, we can conclude that the human-written GPT-4 styled condition is the most challenging from the perspective of training improvement. These results contradict part of our first hypothesis, specifically that post-training accuracy and training improvement would be lowest in LLM-written LLM-styled condition. Other than the difference of the performance of participants in the LLM-written LLM-styled condition, the predicted order of the difficulty of each condition in terms of training improvement and training speed was correct. For this reason, we used the most difficult Human-written LLM-styled emails in our second experiment on methods of increasing training improvement. }

\subsubsection{Training Speed.} 
The right side of Figure \ref{fig:trainingExp1} compares each condition in terms of the training speed metric previously defined. From this comparison, we can see that the human-written LLM-styled condition additionally has the slowest training speed among all conditions. Only the human-written LLM-styled condition was significantly lower than another condition in experiment 1, the LLM-written LLM-styled condition, which had the fastest average training speed. The following statistical analysis was performed to confirm the significance of the difference in training speed. 

A two-way ANOVA comparing the effect of author and style on training speed revealed that there was a statistically significant difference in training speed between author (F(1,212)=8.506, p=0.004, $\eta^2$=0.039), no difference between style (F(1,212)=0.092, p=0.762, $\eta^2$=0), and a significant interaction effect (F(1,212)=9.640, p=0.002, $\eta^2$=0.043). Tukey's HSD test for multiple comparisons found that the mean value of the training speed was significantly lower in the human-written LLM-styled condition compared to the LLM-written LLM-styled condition (p=0.042, diff=-23.029 se=8.674, T=-2.655, hedges=-0.508). From this analysis, we can conclude that the human-written LLM-styled condition is significantly more difficult in terms of both training improvement and training speed. 

\subsubsection{Average Performance by AI Generation Perception and Phishing Experience}
Figure \ref{fig:figure5} presents the correlations between average performance based on participant's perception of the emails they observed as AI generated (far left and middle left), and their previous phishing experience (middle right and far right). Each point in the figure represents one participant, with the regression line fit to all data points. We find a significant correlation between the phishing experience and the average performance ($r=-0.40$,$p<0.001$) and the perception of AI generation ($r=0.48$,$p<0.001$) under all the conditions of Experiment 1. The presence of these relationships of AI generation perception and phishing experience on average performance raises the question of whether this effect on AI generation perception and phishing experience on average performance occurs for different types of emails. The following statistical analysis tests whether this correlation is significant for emails authored by humans compared to LLMs, and plaintext-styled emails compared to LLM-styled emails.

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig7.pdf}
  \caption{For all subfigures each point represents one participant, line is a linear regression, and shaded region represents 95\% confidence interval. Far Left: participant average performance compared to their perception of content as being AI generated, in the two conditions with human-written emails. Middle Left: Participant average performance compared to their perception of emails as being AI generated, in the two conditions with LLM-written emails. Middle Right: Participants average performance compared to their phishing experience in the two plaintext-styled conditions. Far Right: Participant average performance compared to their phishing experience in the two LLM-styled conditions.}
  \label{fig:figure5}
\end{figure}

\textbf{AI Generation Perception}:  We test whether emails generated by an LLM have the same bias in AI generation perception compared to emails written by humans. We first performed a two-way ANOVA to compare the effect of AI generation perception and author on average performance, this revealed a significant effect of AI generation perception (F(1,212)=6.707, p=0.010, $\eta^2$=0.035) and author (F(1,212)=415.484, p<1e-4, $\eta^2$=0.971), though no significant interaction effect (F(1,212)=1.351, p=0.176, $\eta^2$=0.099). We next performed two separate one-way ANOVAs on the effect of AI generation perception on average performance were limited to the LLM author conditions and the CLLM-feedback conditions separately. These results revealed a significant effect of AI generation perception on average performance in both the human author conditions (F(14,212)=3.626, p<1e-4, $\eta^2$=0.358) and in the LLM author conditions (F(14,212)=1.870, p=0.040, $\eta^2$=0.218). This demonstrates that the observed AI generation perception bias occurs irrespective of the email author. 

\textbf{Phishing Experience}:  We tested the effect of the phishing experience to determine whether the phishing experience of the participant had the same impact on average performance under different conditions. We first performed a two-way ANOVA to compare the effect of phishing experience and email style on average performance. Results revealed a significant effect of phishing experience (F(13,212)=301.917, p<1e-4, $\eta^2$=0.954) and style (F(13,212)=301.917, p<1e-4, $\eta^2$=0.954), though no significant interaction effect (F(13,212)=0.733, p=0.730, $\eta^2$=0.048). Two separate one-way ANOVAs on the effect of phishing experience on average performance, limiting the analysis to the LLM-styled conditions and the plaintext-styled conditions separately, revealed a significant effect of phishing experience on average performance in the LLM-styled conditions (F(13,147)=6.229, p<1e-4, $\eta^2$=0.355) but not in the plaintext-styled conditions (F(9,144)=2.074, p=0.053, $\eta^2$=0.298). These analyses demonstrate that the impact of the phishing experience on average performance occurs across the entire experiment and in the LLM-styled conditions separately, but is not significant when looking at only plaintext-style conditions. 

\red{Together, this analysis of participant performance by AI Generation Perception and Phishing Experience support our second hypothesis that participants with more experience with phishing emails or AI models would have better learning outcomes. The analysis revealed patterns consistent with this hypothesis, identifying two specific subgroups, those with less AI experience and those with less Phishing experience, who could benefit from improvements in training methods. The effect on training outcomes of participants who have similar experiences is tested in our second experiment on methods to improve training outcomes.}

\subsection{Experiment 2}
\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig8.pdf}
  \caption{Training improvement and training speed measures for each condition in experiment 2. For both measures, higher represents better training outcomes for students. Error bars represent standard deviation. * represents p<0.05, ** represents p<0.01, *** represents p<0.001.}
  \label{fig:Figure6}
\end{figure}

\subsubsection{Comparing Training Outcomes by Condition}
The results in Figure \ref{fig:Figure6} show the training improvement and training speed across conditions of experiment 2. From these we can see that the CLLM-selection and CLLM-feedback conditions have significantly higher training improvement compared to both the random-selection point feedback condition and the CLLM-selection point feedback condition. This demonstrated that our method of improving training using the CLLM framework results in significantly higher training outcomes as measured by training improvement. Meanwhile, only the CLLM random-selection feedback condition has significantly higher training speed compared to the other conditions. This makes intuitive sense when we consider that the CLLM email selection method intentionally chooses emails that are predicted to be more challenging for students, resulting in a slower training speed but a higher training improvement. The remainder of this section provides statistical analysis to support these conclusions on the relative training outcome measures between conditions. 

\textbf{Training Improvement:} We are interested in determining which of the conditions is the most challenging learning task so that we can use exclusively the emails in experiment 2 in our comparison of approaches to improve training results. To do this, we investigated the impact of the condition of experiment 2 on training improvement. A two-way ANOVA was performed to compare the effect of the email selection strategy and the feedback method on training improvement revealed a main effect of feedback (F(1,2)=533.541, p<1e-4, $\eta^2$=0.716) and selection (F(1,2)=1289.774, p<1e-4, $\eta^2$=0.859), as well as an interaction effect (F(1,4)=297.682, p<1e-4, $\eta^2$=0.584). Tukey's HSD test for multiple comparisons found that the mean value of training improvement was significantly higher in the CLLM-selection CLLM-feedback condition compared to the random-selection point feedback condition (p=0.031, diff=10.107 se=3.651, T=2.769, hedges=0.666). There were no statistically significant differences between each other in comparison of the experiment conditions. From these results, we can conclude that the CLLM-selection CLLM-feedback condition resulted in the largest improvement in training performance of all the conditions in experiment 2.

\textbf{Training Speed:} While the CLLM-selection CLLM-feedback method demonstrated the highest training improvement, there is a question of whether or not selecting emails to be challenging for students slows down training speed. To determine this, a two-way ANOVA was performed to compare the effect of the email selection strategy and the feedback method on the training speed revealed no main effect of feedback (F(1,2)=533, p<0.17, $\eta^2$=0.716) but an effect of selection (F(1,2)=1289.774, p<1e-4, $\eta^2$=0.859), as well as an interaction effect (F(1,4)=297.682, p<1e-4, $\eta^2$=0.584). Tukey's HSD test for multiple comparisons found that the mean value of training improvement was significantly higher in the CLLM-selection CLLM-feedback condition compared to the random-selection point feedback condition (p = 0.049, diff = 23.115 se = 8.889, T = 2.600, hedges = 0.512). There was no statistically significant difference between the other comparison of the experimental conditions. From these results, we can conclude that the CLLM-selection CLLM-feedback condition resulted in the largest improvement in training performance of all the conditions in experiment 2.

\subsubsection{Comparing Average Performance by Condition}
\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig9.pdf}
  \caption{For all subfigures each point represents one participant, line is a linear regression, and shaded region represents 95\% confidence interval. Far Left: Participant average performance by phishing experiment in the random-selection conditions. Middle Left: Participant average performance by phishing experience for the CLLM email selection conditions. Middle Right: Participant average performance by AI generation perception for the point feedback conditions. Far Right: Participant average performance by AI generation perception for the CLLM-feedback conditions.  }
  \label{fig:Fig9}
\end{figure}

\textbf{Phishing Experience}:  We are interested in the relative impact of the phishing experience on the average performance between different types of selection, to see whether receiving email selection from the CLLM model reduces the effect observed in experiment 1. This comparison is shown in Figure \ref{fig:Fig9}. We first performed a two-way ANOVA to compare the effect of phishing experience and selection on average performance, which revealed a significant effect of phishing experience (F(10,110)=534.795, p<1e-4, $\eta^2$=0.964) and selection (F(10,110)=534.795, p<1e-4, $\eta^2$=0.964), but no interaction effect (F(10,110)=1.336, p=0.213, $\eta^2$=0.063). After this, we performed two separate one-way ANOVAs on the effect of phishing experience on average performance, limiting the analysis to the random-selection conditions and the CLLM-selection conditions separately. This analysis revealed a significant effect of phishing experience on average performance under random-selection conditions (F(9,183)=5.888, p<1e-4, $\eta^2$=0.376) but not in the CLLM-selection conditions (F(8,76)=0.256, p=0.978, $\eta^2$=0.026).

\textbf{AI Generation Perception}:  We are interested in the relative impact of AI generation perception on average performance between different types of feedback, to see whether receiving feedback from an AI chatbot reduces the effect observed in experiment 1. We first performed a two-way ANOVA to compare the effect of AI generation perception and feedback on average performance, this revealed a significant effect of AI generation perception (F(14,61)=439.728, p<1e-4, $\eta^2$=0.976) and feedback (F(14,61)=439.728, p<1e-4, $\eta^2$=0.976), as well as an interaction effect (F(14,183)=2.645, p=0.002, $\eta^2$=0.194). After this, we performed two separate one-way ANOVAs on the effect of AI generation perception on average performance, limiting the analysis to the point feedback conditions and the CLLM-feedback conditions separately. This analysis revealed a significant effect of AI generation perception on average performance in the point feedback conditions (F(14,183)=3.528, p<1e-4, $\eta^2$=0.379) but not in the CLLM conditions (F(13,73)=1.302, p=0.232, $\eta^2$=0.188).

%\red{\textbf{Relation to Hypothesis 3:}}
\red{The analysis of participant training outcomes by condition and their average performance by condition support our third hypothesis that the most significant benefits to training improvement would be observed in the CLLM-selection CLLM-feedback condition. However, the training speed of participants in the Random Selection CLLM-Feedback condition was the highest. While this went against our predictions, it makes intuitive sense as the CLLM-selection conditions intentionally choose more difficult emails, which results in slower training speed. Thus, we identified different methods for differentially improving training outcomes based on the specific learning goals that are desired.}

\subsubsection{Experiment 2 AI Identification}
One interesting difference between the association of AI identification and average performance is that CLLM e-mails selected with CLLM-written feedback do not have the same negative trend between the two measures. To compare this statistically, we performed a mixed effects analysis of variance of the effect of condition and AI Identification on average performance. This demonstrated a significant variation of the effects of AI identification (F(14,160)=1951.744, p<1e-4, $\eta^2$=0.994), with no main effect of condition on average accuracy (F(4,160)=-2.379e-11, p=1, $\eta^2$=-4.94e-13), and a significant interaction effect (F(60,160)=11.769, p<1e-4, $\eta^2$=0.8152). A post-hoc Tukey HSD test was performed by binning the AI generation perception into high and low by 50\%. This revealed that in the random selection and point feedback condition lower AI generation was associated with significantly higher performance (p<1e-4, diff=18.52, se=0.0321, T=5.7628, hedges=1.5622). Meanwhile, the same post-hoc Tukey HSD performed by again binning AI generation perception into high and low by 50\% revealed no significant difference between high and low AI generation perception participants (p=0.311, diff=2.79, se=0.0272, T=1.024, hedges=0.3337). This difference between the base condition and the CLLM condition suggests that one possible mechanism for the improved performance of students in the CLLM condition is due to a reduction in the impact of AI identification bias on average performance.  

\subsection{Ablation Experiment}
The final set of analysis compares the importance of the integration of the IBL and LLM model with an alternative condition that additionally selects example emails and provides natural language feedback, but without the IBL-LLM connection. This ablation condition selected emails using an IBL model based on email features, not email embeddings, and additionally provided natural language feedback using an LLM, but without prompting information from the IBL model. This ablation condition is referred to as the LLM-feedback condition of IBL selection. To this end, the following statistical analysis adds the ablation condition to the analysis of experiment 2 conditions. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{Figures/Fig10.pdf}
  \caption{Far Left: Average participant training improvement by the three conditions used for comparison in the ablation experiment. Middle Left: Average participant training speed by the three conditions used for comparison in the ablation experiment. Middle Right: Regression plot of IBL selection LLM-feedback condition participant average performance by AI generation perception. Far Right: Regression plot of IBL selection LLM-feedback condition participant average performance by phishing experience. Regression plot points represent one participant, shaded regions represent 95\% confidence interval. Bar plot whiskers represent 95\% confidence interval. }
  \label{fig:Figure10}
\end{figure}

The results in Figure \ref{fig:Figure10} on the two left subplots compare the training improvement and the training speed of the IBL selection LLM-feedback condition with the condition of experiment 2 that had the highest training improvement, CLLM-selection CLLM-feedback, and the condition that had the highest training speed, Random selection CLLM-feedback. From this comparison, we can see that the CLLM-selection CLLM-feedback condition has a significantly higher training improvement compared to the ablated condition that did not have the same connection between the IBL cognitive model and LLM. Furthermore, the high training speed observed when randomly selecting emails and providing CLLM-feedback was not achieved under the ablated condition. From these comparisons, we can see that the benefits of our CLLM are in part due to the connection between the IBL cognitive model and the LLM. 

In the two right subplots of Figure \ref{fig:Figure10}, we compare the regressions of average performance with perception of AI generation (middle right) and the experience of phishing (far right). These regressions show a significant correlation, in the same direction, and roughly equivalent rates as with the previous conditions. Comparing this to the regression analysis in the previous section, we can see that using IBL to select emails and an LLM to provide feedback separately does not have the same effect on reducing the biases that we identified as impacting participant performance. From this analysis, we can conclude that an important aspect of the observed reduction in participant biases that was attained by our CLLM is due to the connections between the IBL cognitive model and LLM. The remainder of this section provides statistical analysis to further support these conclusions. 

The results of a one-way ANOVA had a significant difference in the ablation experiment and the CLLM-feedback and selection condition. (F(1,79)=8.242, p=0.005, $\eta^2$=0.094) A follow-up Tukey HSD demonstrated that the random-selection CLLM-feedback had a faster training speed compared to the IBL selection LLM-feedback condition (p<1e-4, diff=0.121 se=0.023, T=5.325, hedges=0.967), while no other condition had a significant variation. This makes intuitive sense, as the CLLM and IBL email selection method intentionally choose difficult emails for students. However, the motivation to select emails using the CLLM is to increase the difficulty in a way that results in more significant improvements in training outcomes. 

To determine whether the CLLM-selection method has this expected effect on training improvement, we next compared this condition to the ablation condition in terms of training improvement. A one-way ANOVA demonstrated a significant variation in the effect of the experimental condition on training improvement (F(1,79)=8.242, p=0.0052, $\eta^2$=0.0944). A follow-up Tukey HSD demonstrated that the CLLM-selection and CLLM-feedback condition had a significantly higher training improvement compared to the IBL selection LLM-feedback condition (p=0.045, diff=10.107 se=3.625, T=2.788, hedges=0.666). This identifies the CLLM-selection and the CLLM-feedback condition as superior in terms of training improvement, while the random-selection CLLM-feedback condition is superior in terms of training speed.

%\red{\textbf{Relation to Hypothesis 3:}}
\red{The ablation experiment and analysis of the performance of participants that perceived AI written emails as being more likely to be phishing demonstrated that the CLLM-selection CLLM-feedback condition most significantly improved the training outcomes of our target subpopulations. Specifically, they demonstrate that increases in training outcomes can be provided to participants who have less experience with AI or have less experience with phishing emails. This is a crucial result, as the benefits of the training outcomes that ignore these important populations could lead to unjust educational approaches.}

\section{Discussion} 
To address these expanding threats, we propose and evaluate a novel Cognitive Large Language Model (CLLM) framework to train end users in the detection of these emails in a personalized way. The framework integrates an Instance-Based Learning (IBL) cognitive model with LLM and uses techniques inspired by tutoring systems to trace the end uses. This framework significantly improved the ability of students to correctly categorize the most challenging emails by selecting educational examples and providing personalized contextual feedback. Compared to alternatives such as randomly selecting emails or providing generic feedback, the CLLM framework leveraged participant-specific learning data to effectively improve participant training.

Previous studies found that fully LLM-generated phishing emails, such as those created by GPT-3, were generally easier for humans to detect than emails written by human experts \citep{sharma2023well}, leading to the belief that AI-generated phishing emails might be less effective for phishing education scenarios. In contrast, the current study demonstrates that while fully LLM-generated emails remain relatively easier for students to identify, integrating human-written content with LLM-stylized elements results in significantly more convincing phishing emails. This highlights a shift in the threat landscape, emphasizing the potential danger of adversaries using LLMs not as standalone tools but in conjunction with human expertise to craft more deceptive and harmful content. This result represents the first main contribution of this work. 

The next main contribution of this work is a follow-up experiment where we compared different methods of integrating LLMs into anti-phishing training, to mitigate the dangers associated with human cyberattackers leveraging LLMs. Alongside this experiment, we also propose and introduce a novel Cognitive LLM framework that integrates a cognitive model of human learning and decision making in an attempt to improve training outcomes. Results from this experiment show that our proposed CLLM can improve training outcomes, and aid in effectively mitigating some of the dangers associated with the misuse of LLMs. 

Another advantage of our framework compared to previous work is the use of personalizing feedback based on individual learning trajectories. Earlier research has raised concerns about biases in LLM-generated content, such as hallucinations or overly generic feedback \citep{huang2023survey, kasneci2023chatgpt}, while other work has shown that different students can benefit from different levels of detail in anti-phishing training \citep{singh2023cognitive}. Our approach addresses this problem of tailoring details and information to each individual in a conversational manner. \red{This results in reduction of biases in learning outcomes based on students familiarity with phishing emails. Similarly, we find a reduction in an observed bias toward categorizing AI-generated images as phishing, which is related to previous research in human aversion to trust in AI \citep{sharma2023well} and the ethical concerns of LLMs with biases that could unfairly impact learning outcomes \citep{roy2024chatbots}.}

Furthermore, these results demonstrate that different approaches to the integration of CLLM in training platforms can result in different types of training improvement. The selection of emails using the CLLM showed slower training speeds, but more significant training improvements, while the feedback with the CLLM improved both the training speed and the overall quality of training. An important source of these improvements identified in the results of the experiment is that CLLM feedback and selection conditions were not as negatively impacted by the low phishing experience or content perception as AI generated (see Figure \ref{fig:Fig9}).

Our findings have broader implications for understanding the role of AI in online education and cybersecurity. Our study demonstrates that LLMs, when integrated with cognitive models, can actively support adaptive and personalized education in complex domains. This integration is particularly valuable in combating the misuse of these AI tools in the phishing domain, where adversaries can collaborate their work with AI tools for malicious purposes. By addressing the gaps in current training methodologies, the framework proposed in this work effectively provides proactive risk mitigation against AI misuse.

\red{The interaction between humans and LLMs is constantly evolving, with significant efforts in designing LLMs that are \textit{agentic} or able to make decisions and take actions \citep{qiu2024llm}. This raises the question of how LLMs might be misused to perform nefarious actions, and how we can leverage more agentic LLMs in mitigating these cases of misuse. The results in this work demonstrate that in some cybersecurity applications, this Human-LLM interactive misuse is more significantly impactful when humans work with LLMs to produce phishing emails. Additionally, we show that our proposed CLLM model can effectively mitigate the Human-LLM model interactive misuse by improving educational platforms for anti-phishing training}.

\red{\subsection{Limitations to current methodology}}
\red{One limitation of conducting research about LLMs is their ever adapting nature, with new versions of black-box online accessible models such as ChatGPT being updated several times each week \citep{ChatGPTReleaseNotes}. As a result of our desire to use the most up-to-date model, we chose to use GPT-4 for the generation of phishing emails, but used GPT-4o for communication with students as it had recently become available. This issue of reproducibility is partially mitigated by the inclusion of version number information in this work; however, previous versions of GPT are not perpetually available through the OpenAI API \citep{openai_api_documentation}.} 

\red{Another related limitation is the recent explosion in the popularity of these online LLM chatbots, which have been integrated into popular search platforms and websites in the form of conversational agents and personal assistants \citep{casheekar2024contemporary}. As a result, there could be a changing landscape of the biases people have about the content generated by LLMs \citep{fan2025user}. In this work, we observed a significant bias that leads to worse training outcomes in conditions with LLM generated emails, and a reduction of this bias when using our CLLM. However, in the future, this bias may be less pronounced \citep{pmlr-v235-huang24x}, meaning that the difference in the improvement of training outcomes afforded by our model would be reduced. Future research is necessary to test the resilience of this AI bias and to determine the benefits of the CLLM framework.}
\vspace{10pt}
\section{Conclusion}
The proposed Cognitive-Large Language Model (CLLM) framework represents a significant advancement in adaptive, personalized phishing education. By integrating Instance-Based Learning (IBL) with LLMs, this approach improves both the selection of challenging training examples and the delivery of tailored, context-aware feedback. Unlike traditional methods, which rely on fixed datasets or generic feedback, the CLLM dynamically adapts to individual learning trajectories, mitigating biases linked to AI generation perception and prior phishing experience. This personalized approach not only improves educational results, but also addresses limitations identified in earlier work, such as the inability of generic LLM-feedback to accommodate diverse learning needs.

These findings demonstrate the potential of integrating cognitive models with LLMs to advance online education and counter the misuse of AI in cybersecurity. Although previous research has focused on improving LLM alignment and safety, our work emphasizes the importance of enabling LLMs to individualize their responses by prompting them with information obtained from cognitive models. By reducing biases and improving learning, the CLLM framework offers a scalable and practical solution to mitigating the risks posed by hybrid human-AI phishing attacks. This integration of cognitive and generative models demonstrates a successful application of leveraging AI to address challenges in cybersecurity, and has potential future applications in a variety of related domains.

\section{Acknowledgment}
This research was sponsored by the Army Research Office and accomplished under Australia-US MURI Grant Number W911NF-20-S-000, and the AI Research Institutes Program funded by the National Science Foundation under AI Institute for Societal Decision Making (AI-SDM), Award No. 2229881. Compute resources and GPT model credits were provided by the Microsoft Accelerate Foundation Models Research Program grant ``Personalized Education with Foundation Models via Cognitive Modeling"

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{bib}
% Loading bibliography database
\bibliography{bib}
\end{document}

