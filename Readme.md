# Foundation Phishing: Experiment, Data, and Codebase

This repository contains the materials for the project “Improving Online Training to Identify Phishing Emails Generated by Humans and LLMs,” including the web-based experiment, participant dataset, statistical analyses, and the manuscript/figures.

Below is a brief overview of the experiment, the available data, the code layout, and direct links to view the figures.

## Experiment overview

- Web-based training and test tasks where participants classify emails as phishing or legitimate and report confidence and response time.
- Stimuli include both human-written and LLM-generated emails across conditions and phases (e.g., training vs. test).
- Some conditions provide an assistive chatbot interface to help users reason about the emails (see `WebsiteDemo/public/html/gptChat.html` and `WebsiteDemo/public/html/pointchat.html`).
- Outcomes tracked include decisions, correctness, reaction times, confidence, conversation transcripts (when applicable), and demographics.

For a concise description of the logged data fields used in the analyses, see the “Dataset” section below and `Analysis/ReadMe.md`.

## Dataset

The canonical participant-level dataset is provided in multiple formats under `Data/`:

- `Data/ParticipantData.csv`
- `Data/ParticipantData.json`
- `Data/ParticipantData.mat`
- `Data/ParticipantData.pkl`
- `Data/ParticipantData.xlsx`

Key columns (see `Analysis/ReadMe.md` for details and exact spellings):

- UserId, Experiment, EmailId, PhaseTrial, ExperimentTrial
- DataType (e.g., decision vs. message), Decision, MessageNum, Message
- EmailType, PhaseValue, ExperimentCondition, Confidence
- EmailAction, ReactionTime, Correct
- Age, Gender, Education, Country, Victim, Chatbot, Consent
- Q0–Q5, PQ0–PQ5, Rejected
- Training Speed, Pretraining Accuracy, Categorization Improvement
- AI Generation Perception, AI Perception Accuracy, Baseline Phishing Knowledge
- Author, Style, Feedback, Selection

These fields support reproducing the figures and statistics in the paper and supplement.

## Codebase structure

- `WebsiteDemo/` – The web experiment implementation.
	- `app.mjs` runs an Express server and sockets.
	- `public/` includes HTML pages, CSS, images, and client JS for experiments (`experiment1.js`–`experiment4.js`) and chatbot UIs.
	- Requires Node.js and a few environment variables (Azure OpenAI credentials) if you use the chatbot conditions.

- `Analysis/` – Python code for data cleaning, statistics, and figure generation.
	- `Analysis/requirements.txt` lists pinned dependencies (pandas, seaborn, statsmodels, scikit-learn, pyibl, etc.).
	- `Analysis/Figures/` contains scripts such as `Figure4.py`–`Figure8.py`, `Results.py`, and `Statistics.py` (and an `old/` archive).
	- `Analysis/Data/` contains helper assets and a placeholder `Readme.md`.

- `Figures/` – Final, exported figures used by the LaTeX manuscript (PDF/PNG).

- `Latex/` – Manuscript source (`cas-sc.tex`, `supplementary-materials.tex`, and bibliography).

### Reproducing analyses and figures

1) Create a Python environment and install dependencies:

```
pip install -r Analysis/requirements.txt
```

2) Run individual figure scripts or the results driver from the `Analysis/Figures/` directory, for example:

```
python Analysis/Figures/Figure4.py
python Analysis/Figures/Figure5.py
python Analysis/Figures/Results.py
```

Outputs are written to the `Figures/` folder.

## Figure gallery (click to view)

Note: The figures used by the LaTeX manuscript are stored in the top-level `Figures/` folder.

- [Figure 1 (PDF)](Figures/Fig1.pdf)
- [Figure 2 (PDF)](Figures/Fig2.pdf)
- [Figure 3 (PDF)](Figures/Fig3.pdf)
- [Figure 4 (PDF)](Figures/Fig4.pdf)
- [Figure 5 (PDF)](Figures/Fig5.pdf)
- [Figure 6 (PDF)](Figures/Fig6.pdf)
- [Figure 7 (PDF)](Figures/Fig7.pdf)
- [Figure 8 (PDF)](Figures/Fig8.pdf)
- [Figure 9 (PDF)](Figures/Fig9.pdf)
- [Figure 10 (PDF)](Figures/Fig10.pdf)
- [Supplemental image (PNG)](Figures/image1.png)

## Web experiment (WebsiteDemo)

### Prerequisites

- Node.js (LTS)
- If using chatbot-enabled conditions, set these environment variables (Azure/OpenAI access):
	- `AZURE_OPENAI_ENDPOINT`
	- `AZURE_OPENAI_KEY`
	- `AZURE_DB_PASSWORD`
	- `NODE_TLS_REJECT_UNAUTHORIZED`

You can temporarily log these in `app.mjs` (commented out) to confirm they are visible to the app.

### Run locally

From `WebsiteDemo/`:

```
npm install
npm run start        # macOS / Linux
npm run start-pc     # Windows
```

Then open http://127.0.0.1:8888/ (localhost:8888) to load the experiment.

### Notes on implementation

- Built with [Node.js](https://nodejs.org/en/), [express](https://expressjs.com/), [socket.io](https://socket.io/), and the [OpenAI](https://platform.openai.com/docs/libraries) SDK (when using chatbot conditions).
- Client-side tasks leverage [JsPsych](https://www.jspsych.org/7.3/).
- Main static assets live in `WebsiteDemo/public/` (HTML/CSS/JS).

## Manuscript

- LaTeX sources: `Latex/cas-sc.tex` and `Latex/supplementary-materials.tex`.
- Bibliography: `Latex/bib.bib` and `Latex/bib.bst`.

## Provenance

- “Frontiers 2024”: Files for a submission on GAI in cognitive modeling.
- “CogSci 2024”: Simulation files for a submission on simulating participant learning with IBL and GPT.
